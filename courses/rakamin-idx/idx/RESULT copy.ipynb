{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "from plotly.subplots import make_subplots\n",
    "from ydata_profiling import ProfileReport\n",
    "import seaborn as sns\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"loan_data_2007_2014.csv\", index_col=0, low_memory=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_report = ProfileReport(df, minimal=True)\n",
    "# df_report.to_file(output_file=\"1-profiling.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loan_status.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"loan_status\"] != \"Current\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop unused columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constant\n",
    "df.drop(columns=[\"policy_code\", \"application_type\"], inplace=True)\n",
    "\n",
    "# Unique\n",
    "df.drop(columns=[\"id\", \"member_id\", \"url\"], inplace=True)\n",
    "\n",
    "# NLP-related task\n",
    "df.drop(columns=[\"title\", \"desc\"], inplace=True)\n",
    "\n",
    "# Redundant\n",
    "df.drop(columns=[\"funded_amnt_inv\"], inplace=True)\n",
    "\n",
    "# Highly imbalanced\n",
    "df.drop(columns=[\"pymnt_plan\", \"zip_code\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unsupported col\n",
    "unsup_cols = [\n",
    "    \"annual_inc_joint\",\n",
    "    \"dti_joint\",\n",
    "    \"verification_status_joint\",\n",
    "    \"open_acc_6m\",\n",
    "    \"open_il_6m\",\n",
    "    \"open_il_12m\",\n",
    "    \"open_il_24m\",\n",
    "    \"mths_since_rcnt_il\",\n",
    "    \"total_bal_il\",\n",
    "    \"il_util\",\n",
    "    \"open_rv_12m\",\n",
    "    \"open_rv_24m\",\n",
    "    \"max_bal_bc\",\n",
    "    \"all_util\",\n",
    "    \"inq_fi\",\n",
    "    \"total_cu_tl\",\n",
    "    \"inq_last_12m\",\n",
    "]\n",
    "\n",
    "df.drop(columns=unsup_cols, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_value_count = df.isnull().sum()\n",
    "\n",
    "# Print the null value count for each column\n",
    "print(null_value_count)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fillna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"emp_title\"].fillna(\"N/A\", inplace=True)\n",
    "df[\"emp_length\"].fillna(\"N/A\", inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"term\"] = df[\"term\"].replace({\" 36 months\": 36, \" 60 months\": 60})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"loan_status\"] = df[\"loan_status\"].replace(\n",
    "    {\n",
    "        \"Does not meet the credit policy. Status:Fully Paid\": \"Fully Paid\",\n",
    "        \"Does not meet the credit policy. Status:Charged Off\": \"Charged Off\",\n",
    "        \"Late (31-120 days)\": \"Late\",\n",
    "        \"Late (16-30 days)\": \"Late\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract date feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_date = [\n",
    "    \"issue_d\",\n",
    "    \"earliest_cr_line\",\n",
    "    \"last_pymnt_d\",\n",
    "    \"next_pymnt_d\",\n",
    "    \"last_credit_pull_d\",\n",
    "]\n",
    "\n",
    "for col in col_date:\n",
    "    df[col] = pd.to_datetime(df[col], format=\"%b-%y\")\n",
    "    df[col + \"_month\"] = df[col].dt.month\n",
    "    df[col + \"_year\"] = df[col].dt.year"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ori = df.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df[[\"loan_amnt\", \"funded_amnt\", \"int_rate\", \"installment\"]])\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loan amount over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_monthly = df.set_index(\"issue_d\")\n",
    "\n",
    "# Aggregate loan amounts by month\n",
    "loan_amount_monthly = df_monthly[\"loan_amnt\"].resample(\"M\").sum()\n",
    "\n",
    "# Count the number of loans by month\n",
    "loan_count_monthly = df_monthly[\"loan_amnt\"].resample(\"M\").count()\n",
    "\n",
    "# Combine the aggregated df into a new dfFrame\n",
    "temporal_analysis_df = pd.DataFrame(\n",
    "    {\"Loan Amount\": loan_amount_monthly, \"Loan Count\": loan_count_monthly}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loan amount over time\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(temporal_analysis_df[\"Loan Amount\"])\n",
    "plt.title(\"Loan Amount Over Time\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Loan Amount\")\n",
    "\n",
    "formatter = mticker.FuncFormatter(lambda x, pos: \"{:,.0f}M\".format(x * 1e-6))\n",
    "plt.gca().yaxis.set_major_formatter(formatter)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loan count over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loan count over time\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(temporal_analysis_df[\"Loan Count\"])\n",
    "plt.title(\"Loan Count Over Time\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Loan Count\")\n",
    "\n",
    "# Format y-axis labels\n",
    "formatter = mticker.FuncFormatter(lambda x, pos: \"{:,.0f}K\".format(x * 1e-3))\n",
    "plt.gca().yaxis.set_major_formatter(formatter)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "credit history distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize credit history distribution (earliest_cr_line)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(df[\"earliest_cr_line\"].dt.year, bins=30, kde=True)\n",
    "plt.title(\"Credit History Distribution\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "debt-to-income (DTI) distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df[\"dti\"], kde=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "payment rate over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"issue_d_monthly\"] = df[\"issue_d\"].dt.to_period(\"M\")\n",
    "loan_volume = df[\"issue_d_monthly\"].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize payment rate over time (last_pymnt_d)\n",
    "df[\"last_pymnt_d_monthly\"] = df[\"last_pymnt_d\"].dt.to_period(\"M\")\n",
    "payment_rate = df[\"last_pymnt_d_monthly\"].value_counts().sort_index() / loan_volume\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "payment_rate.plot(kind=\"line\")\n",
    "plt.title(\"Monthly Payment Rate\")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Payment Rate\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "payment status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize payment status (next_pymnt_d)\n",
    "payment_status = df[\"loan_status\"].value_counts()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "payment_status.plot(kind=\"bar\")\n",
    "plt.title(\"Payment Status\")\n",
    "plt.xlabel(\"Payment Status\")\n",
    "plt.ylabel(\"Loan Count\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "\n",
    "Based on the provided dfset, various machine learning tasks can be applied depending on the specific objective or problem you want to solve. Here are some possible tasks that can be performed:\n",
    "\n",
    "1. **Regression**: Predicting a continuous numerical value, such as:\n",
    "   - Predicting the loan amount (`funded_amnt`) or last month's payment received (`loan_amnt`).\n",
    "   - Predicting the monthly installment amount (`installment`).\n",
    "\n",
    "2. **Classification**: Predicting a categorical value, such as:\n",
    "   - Predicting the loan status (`loan_status`) to determine if a borrower will default or repay the loan.\n",
    "   - Predicting the loan grade (`grade` or `sub_grade`), which represents the creditworthiness of the borrower.\n",
    "\n",
    "3. **Binary Classification**: Similar to classification but with only two classes, such as:\n",
    "   - Predicting whether a borrower's income was verified (`verification_status`).\n",
    "   - Predicting whether a borrower's payment plan exists (`pymnt_plan`).\n",
    "\n",
    "4. **Sequence Generation**: Generating a sequence of values, such as:\n",
    "   - Predicting the next scheduled payment date (`next_pymnt_d`) based on historical df.\n",
    "\n",
    "5. **Anomaly Detection**: Identifying unusual or outlier patterns in the df, such as:\n",
    "   - Detecting borrowers with a significantly higher or lower income (`annual_inc`) compared to others.\n",
    "\n",
    "6. **Clustering**: Grouping similar borrowers based on their features, such as:\n",
    "   - Clustering borrowers based on their employment length (`emp_length`) and annual income (`annual_inc`).\n",
    "\n",
    "7. **Feature Importance**: Identifying the most important features that contribute to a specific target variable, such as:\n",
    "   - Determining the key factors that affect the loan status (`loan_status`) or interest rate (`int_rate`).\n",
    "\n",
    "These are just a few examples of the machine learning tasks that can be applied to the given dfset. The choice of task depends on the specific problem you want to solve or the insights you want to extract from the df."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting monthly installment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# # Select the relevant features and target variable\n",
    "# features = ['loan_amnt', 'int_rate', 'term']  # Add more relevant features as needed\n",
    "# target = 'installment'\n",
    "\n",
    "# # Split the df into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(df[features], df[target], test_size=0.2, random_state=42)\n",
    "\n",
    "# # Train the linear regression model\n",
    "# model = LinearRegression()\n",
    "# model.fit(X_train, y_train)\n",
    "\n",
    "# # Make predictions on the test set\n",
    "# y_pred = model.predict(X_test)\n",
    "\n",
    "# # Evaluate the model\n",
    "# mse = mean_squared_error(y_test, y_pred)\n",
    "# print('Mean Squared Error:', mse)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting the loan grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# # Select the relevant features and target variable\n",
    "# features = ['loan_amnt', 'int_rate', 'dti']  # Add more relevant features as needed\n",
    "# target = 'grade'  # or 'sub_grade' for more specific credit rating\n",
    "\n",
    "# # Preprocess the df if needed (e.g., handle missing values, encode categorical variables)\n",
    "\n",
    "# # Split the df into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(df[features], df[target], test_size=0.2, random_state=42)\n",
    "\n",
    "# # Train the logistic regression model\n",
    "# model = LogisticRegression(multi_class='auto')\n",
    "# model.fit(X_train, y_train)\n",
    "\n",
    "# # Make predictions on the test set\n",
    "# y_pred = model.predict(X_test)\n",
    "\n",
    "# # Evaluate the model\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "# print('Accuracy:', accuracy)\n",
    "# print('Classification Report:')\n",
    "# print(classification_rep)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting whether a borrower's income was verified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# # Select the relevant features and target variable\n",
    "# features = ['loan_amnt', 'dti', 'annual_inc']  # Add more relevant features as needed\n",
    "# target = 'verification_status'\n",
    "\n",
    "# # Preprocess the df if needed (e.g., handle missing values, encode categorical variables)\n",
    "# df_n = df[features + [target]].dropna()\n",
    "\n",
    "# # Split the df into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(df_n[features].dropna(), df_n[target], test_size=0.2, random_state=42)\n",
    "\n",
    "# # Train the logistic regression model\n",
    "# model = LogisticRegression()\n",
    "# model.fit(X_train, y_train)\n",
    "\n",
    "# # Make predictions on the test set\n",
    "# y_pred = model.predict(X_test)\n",
    "\n",
    "# # Evaluate the model\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# confusion = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# print('Accuracy:', accuracy)\n",
    "# print('Confusion Matrix:')\n",
    "# print(confusion)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting the next scheduled payment date (`next_pymnt_d`) based on historical df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "# # Select the relevant time series df\n",
    "# time_series_df = df[['next_pymnt_d']]\n",
    "\n",
    "# # Split the df into training and testing sets\n",
    "# train_size = int(len(time_series_df) * 0.8)\n",
    "# train_df, test_df = time_series_df[:train_size], time_series_df[train_size:]\n",
    "\n",
    "# # Train the ARIMA model\n",
    "# order = (1, 1, 1)  # Order (p, d, q) of the ARIMA model\n",
    "# model = ARIMA(train_df, order=order)\n",
    "# model_fit = model.fit()\n",
    "\n",
    "# # Make predictions on the test set\n",
    "# start_index = len(train_df)\n",
    "# end_index = len(time_series_df) - 1\n",
    "# predictions = model_fit.predict(start=start_index, end=end_index)\n",
    "\n",
    "# # Convert the predicted values to the appropriate format if needed\n",
    "\n",
    "# # Print the predicted values\n",
    "# print('Predicted next scheduled payment dates:')\n",
    "# print(predictions)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting borrowers with a significantly higher or lower income (`annual_inc`) compared to others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.ensemble import IsolationForest\n",
    "# # Select the relevant feature for anomaly detection\n",
    "# feature = 'annual_inc'\n",
    "\n",
    "# # Preprocess the data if needed (e.g., handle missing values, normalize the feature)\n",
    "\n",
    "# # Train the Isolation Forest model\n",
    "# model = IsolationForest(contamination=0.05)  # Adjust the contamination parameter as needed\n",
    "# model.fit(df[[feature]])\n",
    "\n",
    "# # Predict anomalies\n",
    "# predictions = model.predict(df[[feature]])\n",
    "\n",
    "# # Identify the indices of anomalies\n",
    "# anomaly_indices = df.index[predictions == -1]\n",
    "\n",
    "# # Print the anomalies\n",
    "# anomalies = df.iloc[anomaly_indices]\n",
    "# print('Anomalies:')\n",
    "# print(anomalies)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering borrowers based on their employment length (`emp_length`) and annual income (`annual_inc`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.cluster import KMeans\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# # Select the relevant features for clustering\n",
    "# features = ['emp_length', 'annual_inc']\n",
    "# columns_to_encode = ['emp_length']\n",
    "\n",
    "# df_f = df[features].dropna()\n",
    "\n",
    "# # Create an instance of the OneHotEncoder\n",
    "# encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "# # Fit and transform the selected column(s) using one-hot encoding\n",
    "# encoded_columns = encoder.fit_transform(df_f[columns_to_encode])\n",
    "\n",
    "# # Create a DataFrame with the encoded columns\n",
    "# encoded_df = pd.DataFrame(encoded_columns, columns=encoder.get_feature_names_out(columns_to_encode))\n",
    "\n",
    "# # Concatenate the encoded DataFrame with the remaining columns\n",
    "# df_encoded = pd.concat([encoded_df, df_f['annual_inc']], axis=1)\n",
    "\n",
    "# # Preprocess the df if needed (e.g., handle missing values, scale the features)\n",
    "# scaler = MinMaxScaler()\n",
    "# df_scaled = scaler.fit_transform(df_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Determine the optimal number of clusters using the elbow method\n",
    "# wcss = []\n",
    "# for i in range(1, 20):\n",
    "#     kmeans = KMeans(n_clusters=i, random_state=42)\n",
    "#     kmeans.fit(df_scaled)\n",
    "#     wcss.append(kmeans.inertia_)\n",
    "# plt.plot(range(1, 20), wcss)\n",
    "# plt.xlabel('Number of Clusters')\n",
    "# plt.ylabel('WCSS')\n",
    "# plt.title('Elbow Method')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train the K-means model with the chosen number of clusters\n",
    "# k = 12  # Adjust the number of clusters based on the elbow method or domain knowledge\n",
    "# kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "# kmeans.fit(df_scaled)\n",
    "\n",
    "# # Assign cluster labels to the df points\n",
    "# df_f['cluster_label'] = kmeans.labels_\n",
    "\n",
    "# # Visualize the clusters (you can modify this based on your features)\n",
    "# plt.scatter(df_f['emp_length'], df_f['annual_inc'], c=df_f['cluster_label'])\n",
    "# plt.xlabel('Employment Length')\n",
    "# plt.ylabel('Annual Income')\n",
    "# plt.title('Clustering of Borrowers')\n",
    "# plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining the key factors that affect the loan status (`loan_status`) or interest rate (`int_rate`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "# from sklearn.inspection import permutation_importance\n",
    "# df = df_ori.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target = 'loan_status'\n",
    "# cat_cols = ['term', 'grade', 'sub_grade', 'emp_title', 'emp_length', 'home_ownership',\n",
    "#            'verification_status', 'purpose','addr_state', 'initial_list_status']\n",
    "# num_cols = list(set(df.columns) - set(cat_cols + [target]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Select the relevant features and target variable\n",
    "# target = 'loan_status'  # Replace with the actual target variable\n",
    "# features = num_cols  # Replace with the actual feature names\n",
    "\n",
    "# # Train a Random Forest model for classification or regression\n",
    "# model = RandomForestClassifier()\n",
    "\n",
    "# model.fit(df[features], df[target])\n",
    "\n",
    "# # Calculate feature importance using permutation importance\n",
    "# perm_importance = permutation_importance(model, df[features], df[target])\n",
    "\n",
    "# # Get the feature importance scores and their corresponding feature names\n",
    "# importance_scores = perm_importance.importances_mean\n",
    "# feature_names = df[features].columns\n",
    "\n",
    "# # Create a feature importance dataframe\n",
    "# feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importance_scores})\n",
    "# feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# # Print the feature importance ranking\n",
    "# print('Feature Importance:')\n",
    "# print(feature_importance_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict credit risk (final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Select relevant columns for credit risk prediction\n",
    "selected_columns = [\n",
    "    \"annual_inc\",\n",
    "    \"loan_status\",\n",
    "    \"grade\",\n",
    "    \"sub_grade\",\n",
    "    \"dti\",\n",
    "    \"inq_last_6mths\",\n",
    "    \"open_acc\",\n",
    "]\n",
    "df = df[selected_columns]\n",
    "\n",
    "# Convert categorical columns to numeric using label encoding\n",
    "categorical_columns = [\"grade\", \"sub_grade\", \"loan_status\"]\n",
    "le = LabelEncoder()\n",
    "for col in categorical_columns:\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "\n",
    "# Split the dataset into features and target variable\n",
    "X = df.drop(\"loan_status\", axis=1)\n",
    "y = df[\"loan_status\"]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create a HistGradientBoostingClassifier\n",
    "clf = HistGradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Train the model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the credit risk on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
