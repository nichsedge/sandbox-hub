{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import json\n",
    "import pytz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "from plotly.subplots import make_subplots\n",
    "from pandas_profiling import ProfileReport\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "import h2o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the JSON file\n",
    "with open(\"data-sample.json\") as f:\n",
    "    df = pd.json_normalize(json.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To datetime\n",
    "datetime_columns = [\"taskCreatedTime\", \"taskCompletedTime\"]\n",
    "df[datetime_columns] = df[datetime_columns].apply(\n",
    "    lambda x: pd.to_datetime(x, format=\"%Y-%m-%d %H:%M:%S %z\", utc=True).dt.tz_convert(\n",
    "        pytz.timezone(\"Asia/Bangkok\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# dtype mapping\n",
    "dtype_mapping = {\n",
    "    \"taskLocationDone.lon\": \"float64\",\n",
    "    \"taskLocationDone.lat\": \"float64\",\n",
    "    \"cod.amount\": \"float64\",\n",
    "    \"cod.received\": bool,\n",
    "    \"UserVar.weight\": \"float64\",\n",
    "}\n",
    "df = df.astype(dtype_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_report = ProfileReport(df)\n",
    "# df_report"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop unused columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop constant values and ID\n",
    "df.drop(columns=[\"flow\", \"taskId\"], inplace=True)\n",
    "\n",
    "# Drop ID\n",
    "df.drop(\n",
    "    columns=[\n",
    "        \"UserVar.taskStatus\",\n",
    "        \"UserVar.taskDetailStatus\",\n",
    "        \"UserVar.taskDetailStatusLabel\",\n",
    "    ],\n",
    "    inplace=True,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop ongoing task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.dropna(subset=['UserVar.taskStatusLabel'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df[df['taskStatus'] == 'done']\n",
    "df.loc[df[\"taskStatus\"] == \"ongoing\", \"UserVar.taskStatusLabel\"] = \"Ongoing\"\n",
    "df.drop(columns=[\"taskStatus\"], inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fillna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"UserVar.branch_origin\"].fillna(\"UNKNOWN\", inplace=True)\n",
    "# df['cod.amount'].fillna(0, inplace=True) # Assumptions: no COD is 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the numeric columns from the DataFrame\n",
    "numeric_columns = df.select_dtypes(include=\"number\")\n",
    "\n",
    "# Iterate over each numeric column\n",
    "for column in numeric_columns.columns:\n",
    "    # Create a new figure for each column\n",
    "    plt.figure()\n",
    "\n",
    "    # Create the boxplot for the current column\n",
    "    sns.boxplot(data=numeric_columns[column])\n",
    "\n",
    "    # Set plot title and labels\n",
    "    plt.title(f\"Boxplot of {column}\")\n",
    "    plt.xlabel(\"Values\")\n",
    "    plt.ylabel(\"Column\")\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"to_ml.csv\", index=False)\n",
    "df_after = deepcopy(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final profiling\n",
    "df_report = ProfileReport(df)\n",
    "df_report"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check unique values\n",
    "object_columns = df.select_dtypes(include=\"object\").columns\n",
    "\n",
    "for column in object_columns:\n",
    "    unique_values = df[column].unique()\n",
    "    print(f\"Unique values in {column}:\")\n",
    "    print(unique_values)\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number\n",
    "attrited_count = df[df[\"UserVar.taskStatusLabel\"] == \"Success\"].shape[0]\n",
    "existing_count = df[df[\"UserVar.taskStatusLabel\"] == \"Failed\"].shape[0]\n",
    "\n",
    "# Create labels and counts for the pie chart\n",
    "labels = [\"Success\", \"Failed\"]\n",
    "counts = [attrited_count, existing_count]\n",
    "\n",
    "# Create the pie chart\n",
    "plt.pie(counts, labels=labels, autopct=\"%1.1f%%\", startangle=90)\n",
    "plt.title(\"Distribution of Task Status\")\n",
    "plt.axis(\"equal\")  # Equal aspect ratio ensures that pie is drawn as a circle\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution of COD Amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_success = df[df[\"UserVar.taskStatusLabel\"] == \"Success\"]\n",
    "df_failed = df[df[\"UserVar.taskStatusLabel\"] == \"Failed\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=df_success, x=\"cod.amount\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=df_failed, x=\"cod.amount\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=df, x=\"cod.amount\", hue=\"UserVar.taskStatusLabel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=df, x=\"UserVar.weight\", hue=\"UserVar.taskStatusLabel\", kde=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "skew data, many outlier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Daily Task Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_count_by_day = (\n",
    "    df.groupby(df[\"taskCreatedTime\"].dt.date).size().reset_index(name=\"Task Count\")\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(\n",
    "    task_count_by_day[\"taskCreatedTime\"], task_count_by_day[\"Task Count\"], marker=\"o\"\n",
    ")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Task Count\")\n",
    "plt.title(\"Daily Task Count\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hourly Task Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hour = df.copy()\n",
    "df_hour[\"Hour\"] = df_hour[\"taskCreatedTime\"].dt.hour\n",
    "\n",
    "# Step 3: Calculate the count of tasks for each hour\n",
    "hourly_task_count = df_hour.groupby(\"Hour\")[\"taskCreatedTime\"].count()\n",
    "\n",
    "# Step 4: Plot the hourly task count\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(hourly_task_count.index, hourly_task_count.values, marker=\"o\")\n",
    "plt.xlabel(\"Hour\")\n",
    "plt.ylabel(\"Task Count\")\n",
    "plt.title(\"Hourly Task Count\")\n",
    "plt.xticks(range(24))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Daily Task Count \"Success vs Fail\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_task_outcome = (\n",
    "    df.groupby(df[\"taskCreatedTime\"].dt.date)[\"UserVar.taskStatusLabel\"]\n",
    "    .value_counts()\n",
    "    .unstack()\n",
    "    .fillna(0)\n",
    ")\n",
    "\n",
    "# Step 4: Plot the daily task success vs fail\n",
    "plt.figure(figsize=(10, 6))\n",
    "daily_task_outcome.plot(kind=\"bar\", stacked=True)\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Daily Task Success vs Fail\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title=\"Task Outcome\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_task_outcome = (\n",
    "    df_hour.groupby(df_hour[\"Hour\"])[\"UserVar.taskStatusLabel\"]\n",
    "    .value_counts()\n",
    "    .unstack()\n",
    "    .fillna(0)\n",
    ")\n",
    "\n",
    "# Step 4: Plot the hourly task success vs fail\n",
    "plt.figure(figsize=(10, 6))\n",
    "daily_task_outcome.plot(kind=\"bar\", stacked=True)\n",
    "plt.xlabel(\"Hour\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Hourly Task Success vs Fail\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title=\"Task Outcome\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average Task Completion Time per Day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the task completion time in hours\n",
    "df[\"completion_time_minutes\"] = (\n",
    "    df[\"taskCompletedTime\"] - df[\"taskCreatedTime\"]\n",
    ").dt.total_seconds() / 60\n",
    "\n",
    "# Group the data by task creation date and calculate the average completion time per day\n",
    "average_completion_time_per_day = df.groupby(df[\"taskCreatedTime\"].dt.date)[\n",
    "    \"completion_time_minutes\"\n",
    "].mean()\n",
    "\n",
    "df_completion_time = pd.DataFrame(\n",
    "    {\n",
    "        \"Date\": average_completion_time_per_day.index,\n",
    "        \"Average Completion Time (minutes)\": average_completion_time_per_day.values,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Plot the average completion time per day\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(\n",
    "    df_completion_time[\"Date\"],\n",
    "    df_completion_time[\"Average Completion Time (minutes)\"],\n",
    "    marker=\"o\",\n",
    ")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Average Completion Time (minutes)\")\n",
    "plt.title(\"Average Task Completion Time per Day\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task Completion Time Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"completion_time_minutes\"].describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_success = df[df[\"UserVar.taskStatusLabel\"] == \"Success\"]\n",
    "plt.title(\"Task Completion Time Distribution\")\n",
    "sns.histplot(df_success[\"completion_time_minutes\"], kde=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Assignment Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most Active workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worker_task_counts = df[\"taskAssignedTo\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top N most active workers\n",
    "top_n_workers = 10  # Set the desired number of top workers\n",
    "most_active_workers = worker_task_counts.head(top_n_workers)\n",
    "\n",
    "print(\"Most Active Workers:\")\n",
    "print(most_active_workers)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most profitable workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = df.groupby(\"taskAssignedTo\")[\"cod.amount\", \"UserVar.weight\"].agg(\n",
    "    [\"sum\", \"mean\", \"count\"]\n",
    ")\n",
    "\n",
    "# Sort by sum of cod.amount in descending order\n",
    "top_sums = grouped_df.sort_values(by=[(\"cod.amount\", \"sum\")], ascending=False)\n",
    "# top_sums\n",
    "top_sums.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(grouped_df[(\"cod.amount\", \"sum\")] / grouped_df[(\"UserVar.weight\", \"sum\")]).sort_values(\n",
    "    ascending=False\n",
    ").head(20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution of Task Completion Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(worker_task_counts, bins=20, kde=True)\n",
    "plt.xlabel(\"Task Completion Count\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Task Completion Counts\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count Completion vs Average Completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worker_performance = df.groupby(\"taskAssignedTo\")[\"completion_time_minutes\"].mean()\n",
    "correlation_df = pd.concat([worker_performance, worker_task_counts], axis=1)\n",
    "correlation = correlation_df.corr()\n",
    "correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_kdeplots(df, col, hue=None):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.histplot(\n",
    "        df[df[\"UserVar.taskStatusLabel\"] == \"Success\"][col],\n",
    "        label=\"Success Task\",\n",
    "        kde=True,\n",
    "        hue=hue,\n",
    "    )\n",
    "    sns.histplot(\n",
    "        df[df[\"UserVar.taskStatusLabel\"] == \"Failed\"][col],\n",
    "        label=\"Failed Task\",\n",
    "        kde=True,\n",
    "        hue=hue,\n",
    "    )\n",
    "    plt.title(f\"Distribution of {col} by Customer Status\")\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in ['UserVar.branch_dest', 'UserVar.taskStatusLabel', 'UserVar.receiver_city',\n",
    "#                        'UserVar.taskDetailStatusLabel', 'UserVar.branch_origin']:\n",
    "#     compare_kdeplots(df, col)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Completion time vs amount vs weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data=df[[\"cod.amount\", \"UserVar.weight\", \"completion_time_minutes\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatter plot with hue\n",
    "sns.scatterplot(\n",
    "    data=df, x=\"cod.amount\", y=\"UserVar.weight\", hue=\"UserVar.taskStatusLabel\"\n",
    ")\n",
    "\n",
    "# Set plot title and labels\n",
    "plt.title(\"Scatter Plot with Hue\")\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Filter relevant columns\n",
    "# filtered_df = df[['taskLocationDone.lon', 'taskLocationDone.lat', 'cod.amount', 'UserVar.taskStatusLabel']]\n",
    "\n",
    "# # Drop rows with missing values\n",
    "# filtered_df.dropna(subset=['taskLocationDone.lon', 'taskLocationDone.lat'], inplace=True)\n",
    "\n",
    "# # Create a folium map centered at the mean coordinates\n",
    "# center_lat = filtered_df['taskLocationDone.lat'].mean()\n",
    "# center_lon = filtered_df['taskLocationDone.lon'].mean()\n",
    "# map = folium.Map(location=[center_lat, center_lon], zoom_start=12)\n",
    "\n",
    "# # Iterate over the filtered dataframe and add markers to the map\n",
    "# for _, row in filtered_df.iterrows():\n",
    "#     lat = row['taskLocationDone.lat']\n",
    "#     lon = row['taskLocationDone.lon']\n",
    "#     amount = row['cod.amount']\n",
    "#     status = row['UserVar.taskStatusLabel']\n",
    "\n",
    "#     # Create marker color based on task status\n",
    "#     color = 'green' if status == 'Success' else 'red'\n",
    "\n",
    "#     # Add marker to the map\n",
    "#     folium.CircleMarker(\n",
    "#         location=[lat, lon],\n",
    "#         radius=5,\n",
    "#         color=color,\n",
    "#         fill=True,\n",
    "#         fill_color=color,\n",
    "#         popup=f\"Amount: {amount}, Status: {status}\"\n",
    "#     ).add_to(map)\n",
    "\n",
    "# # Display the map\n",
    "# map.save('map.html')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Task tatus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = deepcopy(df_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=[\"taskAssignedTo\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"taskCompletedTime\"].fillna(df[\"taskCreatedTime\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = [\"taskLocationDone.lon\", \"taskLocationDone.lat\", \"cod.amount\"]\n",
    "df[col] = df[col].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = [\"UserVar.receiver_city\"]\n",
    "df[col] = df[col].fillna(\"UNKNOWN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"UserVar.taskStatusLabel\"] != \"Ongoing\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract additional time-based features\n",
    "df[\"hourOfDay\"] = df[\"taskCreatedTime\"].dt.hour\n",
    "df[\"dayOfWeek\"] = df[\"taskCreatedTime\"].dt.dayofweek\n",
    "df[\"month\"] = df[\"taskCreatedTime\"].dt.month\n",
    "df[\"year\"] = df[\"taskCreatedTime\"].dt.year\n",
    "df[\"taskDuration\"] = (\n",
    "    df[\"taskCompletedTime\"] - df[\"taskCreatedTime\"]\n",
    ").dt.total_seconds()\n",
    "\n",
    "# Map boolean values to integers\n",
    "df[\"cod.received\"] = df[\"cod.received\"].map({True: 1, False: 0})\n",
    "\n",
    "# Other experiment\n",
    "df[\"taskWeightDurationInteraction\"] = df[\"UserVar.weight\"] * df[\"taskDuration\"]\n",
    "\n",
    "df.drop([\"taskCreatedTime\", \"taskCompletedTime\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode binary variables\n",
    "df[\"status\"] = df[\"UserVar.taskStatusLabel\"].map({\"Success\": 1, \"Failed\": 0})\n",
    "df.drop(columns=[\"UserVar.taskStatusLabel\"], inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = df.select_dtypes(exclude=[\"int\", \"float\"]).columns\n",
    "categorical_columns = [\n",
    "    \"taskAssignedTo\",\n",
    "    \"cod.received\",\n",
    "    \"UserVar.branch_dest\",\n",
    "    \"UserVar.receiver_city\",\n",
    "    \"UserVar.branch_origin\",\n",
    "]\n",
    "categorical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding independent variable x\n",
    "def encode_and_bind(original_dfframe, feature_to_encode):\n",
    "    dummies = pd.get_dummies(original_dfframe[[feature_to_encode]])\n",
    "    res = pd.concat([original_dfframe, dummies], axis=1)\n",
    "    res = res.drop([feature_to_encode], axis=1)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in categorical_columns:\n",
    "    df = encode_and_bind(df, feature)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate x and y sets\n",
    "x = df.drop(\"status\", axis=1).values\n",
    "y = df[\"status\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try raw model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dfset into training set and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, df[\"status\"], test_size=0.2, random_state=1234\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "classifier = XGBClassifier(random_state=1234)\n",
    "classifier.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the test set\n",
    "y_pred = classifier.predict(x_test)\n",
    "\n",
    "# Making the confusion matrix and calculating accuracy score\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "print(accuracy)\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred)\n",
    "auc = metrics.auc(fpr, tpr)\n",
    "print(auc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling imbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing packages for SMOTE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTE(sampling_strategy=\"auto\", random_state=1234)\n",
    "x_sm, y_sm = sm.fit_resample(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Counter(y_train))\n",
    "print(Counter(y_sm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(rows=1, cols=2, specs=[[{\"type\": \"pie\"}, {\"type\": \"pie\"}]])\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Pie(\n",
    "        labels=list(Counter(y_train).keys()),\n",
    "        values=list(Counter(y_train).values()),\n",
    "        name=\"Original df\",\n",
    "    ),\n",
    "    row=1,\n",
    "    col=1,\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Pie(\n",
    "        labels=list(Counter(y_sm).keys()),\n",
    "        values=list(Counter(y_sm).values()),\n",
    "        name=\"SMOTE df\",\n",
    "    ),\n",
    "    row=1,\n",
    "    col=2,\n",
    ")\n",
    "\n",
    "fig.update_traces(textposition=\"inside\", hole=0.4, hoverinfo=\"value+percent+name\")\n",
    "fig.update_layout(\n",
    "    title_text=\"status distribution\",\n",
    "    # Add annotations in the center of the donut pies.\n",
    "    annotations=[\n",
    "        dict(text=\"Original\", x=0.16, y=0.5, font_size=12, showarrow=False),\n",
    "        dict(text=\"SMOTE\", x=0.82, y=0.5, font_size=12, showarrow=False),\n",
    "    ],\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "over = BorderlineSMOTE(sampling_strategy=0.4)\n",
    "under = RandomUnderSampler(sampling_strategy=0.6)\n",
    "\n",
    "steps = [(\"o\", over), (\"u\", under)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(steps=steps)\n",
    "\n",
    "# transform the dfset\n",
    "x_sm_us, y_sm_us = pipeline.fit_resample(x_train, y_train)\n",
    "\n",
    "print(Counter(y_train))\n",
    "print(Counter(y_sm_us))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(Counter(y_train).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(rows=1, cols=2, specs=[[{\"type\": \"pie\"}, {\"type\": \"pie\"}]])\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Pie(\n",
    "        labels=list(Counter(y_train).keys()),\n",
    "        values=list(Counter(y_train).values()),\n",
    "        name=\"Original df\",\n",
    "    ),\n",
    "    row=1,\n",
    "    col=1,\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Pie(\n",
    "        labels=list(Counter(y_sm_us).keys()),\n",
    "        values=list(Counter(y_sm_us).values()),\n",
    "        name=\"SMOTE and US df\",\n",
    "    ),\n",
    "    row=1,\n",
    "    col=2,\n",
    ")\n",
    "\n",
    "fig.update_traces(textposition=\"inside\", hole=0.4, hoverinfo=\"percent+name+value\")\n",
    "fig.update_layout(\n",
    "    title_text=\"status distribution\",\n",
    "    # Add annotations in the center of the donut pies.\n",
    "    annotations=[\n",
    "        dict(text=\"Original\", x=0.16, y=0.5, font_size=12, showarrow=False),\n",
    "        dict(text=\"SMOTE and UnderSample\", x=0.9, y=0.5, font_size=12, showarrow=False),\n",
    "    ],\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = XGBClassifier(random_state=1234)\n",
    "classifier.fit(x_sm, y_sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the test set\n",
    "y_pred = classifier.predict(x_test)\n",
    "\n",
    "# Making the confusion matrix and calculating accuracy score\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "print(accuracy)\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred)\n",
    "auc = metrics.auc(fpr, tpr)\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = XGBClassifier(random_state=1234)\n",
    "classifier.fit(x_sm_us, y_sm_us)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the test set\n",
    "y_pred = classifier.predict(x_test)\n",
    "\n",
    "# Making the confusion matrix and calculating accuracy score\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "print(accuracy)\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred)\n",
    "auc = metrics.auc(fpr, tpr)\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Names of the independent variables\n",
    "feature_names = list(df.drop(\"status\", axis=1).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate train (with resampling) and test sets to build the new dfframe\n",
    "sm_us_x = np.concatenate((x_sm_us, x_test))\n",
    "sm_us_y = np.concatenate((y_sm_us, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_us_df = pd.DataFrame(\n",
    "    np.column_stack([sm_us_y, sm_us_x]), columns=[\"status\"] + feature_names\n",
    ")\n",
    "sm_us_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_clf = RandomForestClassifier(random_state=1234)\n",
    "rf_clf.fit(x_sm_us, y_sm_us)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_plot = 25\n",
    "\n",
    "importances = rf_clf.feature_importances_\n",
    "indices = np.argsort(importances)\n",
    "\n",
    "best_vars = np.array(feature_names)[indices][-features_to_plot:]\n",
    "values = importances[indices][-features_to_plot:]\n",
    "best_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ticks = np.arange(0, features_to_plot)\n",
    "fig, ax = plt.subplots()\n",
    "ax.barh(y_ticks, values)\n",
    "ax.set_yticklabels(best_vars)\n",
    "ax.set_yticks(y_ticks)\n",
    "ax.set_title(\"Random Forest Feature Importances\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_vars = best_vars[-20:]\n",
    "best_vars"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto ML with H2O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from h2o.automl import H2OAutoML\n",
    "\n",
    "h2o.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf = h2o.H2OFrame(sm_us_df[[\"status\"] + list(best_vars)])\n",
    "hf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf[\"status\"] = hf[\"status\"].asfactor()\n",
    "predictors = hf.drop(\"status\").columns\n",
    "response = \"status\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test\n",
    "train, valid = hf.split_frame(ratios=[0.8], seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a Stopping Creterias: max number of models and max time\n",
    "# We are going to exclude DeepLearning algorithms because they are too slow\n",
    "aml = H2OAutoML(\n",
    "    max_models=20, max_runtime_secs=300, seed=1234, exclude_algos=[\"DeepLearning\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "aml.train(x=predictors, y=response, training_frame=train, validation_frame=valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the AutoML Leaderboard\n",
    "lb = aml.leaderboard\n",
    "lb.head(rows=5)  # Print the first 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The model performance in Accuracy: {}\".format(aml.leader.accuracy(valid=True)))\n",
    "print(\"The model performance in AUC: {}\".format(aml.leader.auc(valid=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = aml.leaderboard\n",
    "lb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get GBM model\n",
    "m = h2o.get_model(lb[1, \"model_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.varimp_plot(num_of_features=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in the the Variable Importance plot, the top vars are much the same as they were in the [Feature Selection](#4.-feature-selection) part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.shap_summary_plot(valid)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "https://www.kaggle.com/code/andreshg/churn-prediction-0-99-auc-h2o-sklearn-smote/notebook#4.-Feature-Selection"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
