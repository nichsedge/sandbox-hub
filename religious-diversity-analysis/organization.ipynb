{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install spacy\n",
    "\n",
    "python -m spacy download xx_ent_wiki_sm\n",
    "\n",
    "# python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Make a request to the webpage\n",
    "url = \"https://nasional.kompas.com/read/2017/11/07/18102221/di-sidang-mk-peneliti-lipi-nilai-ahmadiyah-tak-bisa-dianggap-sesat?page=all\"  # Replace with the actual URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Initialize an empty dictionary\n",
    "scraped_data = {}\n",
    "\n",
    "scraped_data[\"title\"] = soup.find(class_=\"read__title\").text\n",
    "scraped_data[\"topic_subtitle\"] = soup.select_one(\".topicSubtitle ul li a\").text\n",
    "scraped_data[\"read_time\"] = soup.select_one(\".read__time\")\n",
    "scraped_data[\"writers\"] = [h6.text for h6 in soup.select(\".credit-title-name\")]\n",
    "scraped_data[\"read_content\"] = \"\\n\".join(\n",
    "    p.text for p in soup.select(\".read__content p\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dictionary to a Pandas DataFrame\n",
    "df = pd.DataFrame([scraped_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"scraped_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the Indonesian model for spaCy\n",
    "nlp = spacy.load(\"xx_ent_wiki_sm\")  # This is the small Indonesian model\n",
    "\n",
    "# Assuming 'read_content' is the variable containing the scraped content\n",
    "doc = nlp(scraped_data[\"read_content\"])\n",
    "\n",
    "# Extract entities\n",
    "entities = [(ent.text, ent.label_) for ent in doc.ents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = set()\n",
    "for entity in entities:\n",
    "    s.add(entity[0] + \"|\" + entity[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in s:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pagination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define the base URL and search parameters\n",
    "scraped_data = []\n",
    "\n",
    "\n",
    "# Function to scrape a single page\n",
    "def scrape_page(query, page):\n",
    "    base_url = \"https://search.kompas.com/search/\"\n",
    "    safe_string = urllib.parse.quote_plus(query)\n",
    "    params = {\n",
    "        \"q\": safe_string,\n",
    "        \"submit\": \"Submit\",\n",
    "        \"gsc.tab\": \"0\",\n",
    "        \"gsc.q\": query.replace(\" \", \"%20\"),\n",
    "        \"gsc.sort\": \"date\",\n",
    "        \"gsc.page\": page,\n",
    "    }\n",
    "\n",
    "    print(params)\n",
    "    response = requests.get(base_url, params=params)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    print(soup.get_text)\n",
    "\n",
    "    # Print the list of hrefs\n",
    "    # print(hrefs)\n",
    "\n",
    "\n",
    "# Process the first page\n",
    "org = \"Front Pembela Islam (FPI)\"\n",
    "scrape_page(org, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "url = \"https://indeks.kompas.com/?site=all&date=2023-11-03&page=1\"\n",
    "req = requests.get(url)\n",
    "\n",
    "# print(req.text)\n",
    "\n",
    "soup = BeautifulSoup(req.text, \"lxml\")\n",
    "\n",
    "a = soup.find_all(\"a\", {\"class\": \"article__link\"})\n",
    "\n",
    "kumpulan_link = []\n",
    "kumpulan_paragraf = []\n",
    "\n",
    "for link in a:\n",
    "    kumpulan_link.append(link[\"href\"])\n",
    "\n",
    "for link in kumpulan_link:\n",
    "    halaman = requests.get(link)\n",
    "    soup_baru = BeautifulSoup(halaman.text, \"lxml\")\n",
    "    paragraf = soup_baru.find_all(\"p\")\n",
    "    for kalimat in paragraf:\n",
    "        kumpulan_paragraf.append(kalimat.text)\n",
    "\n",
    "with open(\"paragraf.txt\", \"a\") as f:\n",
    "    for paragraf in kumpulan_paragraf:\n",
    "        print(\"penulisan berhasil\")\n",
    "        f.writelines(paragraf + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"organization.csv\")\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    # Access each column in the row using row['column_name']\n",
    "    print(row[\"organization_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
